---
title: "Where Have All the .400 Hitters Gone?"
date: '2016-06-26'
author: 'Tom Burr'
layout: post
output: html_document
categories: baseball
---

```{r include = FALSE}
###############################################################################
# Project: Baseball
# Author(s): Thomas Burr
# Date created: 2015 05 26
###############################################################################


################################################################################
# load in libraries and source files
################################################################################


# libraries:
librariesToLoad <- c( 'plyr','tidyr','rmarkdown', 'knitr','ggplot2',
                     'scales','ggthemes','Lahman','dplyr',
                     'gridExtra')

suppressWarnings(suppressMessages(expr={
    sapply(librariesToLoad, function(package) {
        if (require(package, character.only=TRUE)){     
            print(paste(package, "loaded correctly", sep=" "))
        } else {
            print(paste("Installing", package, sep=" "))
            install.packages(package,repos="http://cran.rstudio.com/")
            
            if (require(package, character.only=TRUE)){
              print(paste(package, "loaded correctly", sep=" "))
            } else {
                stop(paste("Could not install", package, sep=" "))
            }
        }
    })
}))


```

###Introduction

Going into the last day of the 1941 season, Ted Williams was sitting on a .39955 average and well clear of the 400 at-bats needed to qualify for the major league batting title. By sitting out, he would have been the first major league player to break the .400 barrier since Bill Terry in 1930. Instead, he played both games of a double header going 6 for 8 and finishing with a .406 average, good for  7th best in the modern era. While this made minor news, it would have been a much bigger deal if anyone could have guessed he would be the last player to do so in 74 years and counting.

All-in all, there were twelve .400+ seasons between 1903 [^1] and 1941, and not one since: 


```{r echo = FALSE}
#Import bstats from the Lahman dataset, using the Lahman R package
bstats <- tbl_df(battingStats())

#Filter to the Modern Era, NL and AL Only
#1903 chosen as the first year when foul-balls counted as strikes. Nap Lajoie hits
#.426 in 1901 before this rule is implemented. 

bstats <- bstats %>% filter(yearID > 1903 & lgID %in% c("NL","AL"))

#Collect player names from Master Dataset, then merge on bstats
master <- tbl_df(Master) %>% select(playerID,nameFirst,nameLast)
bstats <- full_join(bstats,master,by = "playerID")

#Qualifications: Must have at least 400 plate appearances

greatest_seasons <- bstats %>% filter(PA > 400) %>% 
                               arrange(desc(BA)) %>% 
                               mutate(name = paste(nameFirst," ",nameLast)) %>%
                               select(name,BA,yearID)


#Pretty display with kable
kable(greatest_seasons[1:12,],caption = "Twelve .400+ Batting Seasons since 1903",col.names = c("Name","Batting Average","Season"))
```

Baseball's finest hour has passed! There hasn't been a .400 hitter since 1941, and 9 of the ten top seasons happened before 1925! They just don't make 'em like Harry Heilmann anymore!

Obviously this argument doesn't hold up. Batting average is not a solitary statistic, but one composed of his skill relative to the pitchers and fielders against which he plays. 

A more reasonable explanation could be that pitchers and fielders have simply improved at a rate faster than batters, leading to an overall decline in batting average. According to Stephen Gould, this doesn't quite tell the full story either. Here he is on the supposed "disappearance" of the .400 hitter:


> The overall batting average has been about .260 throughout the history of baseball. But the variation around that average has shrunk. It's at least plausible that variation declines because play improves. A batting average is a comparison between hitting and pitching. So if everybody's improving, as long as they improve at the same rate, the batting average will remain constant. But it gets to the point where everyone is so good that there's just not much variation anymore. Hitting .400 in baseball is a good example because there's a "right wall," if you will, of human limits. Given how our muscles work, there's just so much that the human body can do. There will always be a few individuals who, by dint of genetic gifts and obsessive commitment and training, will stand close to that right wall. That's where Ty Cobb was in 1911 and where Tony Gwynn is today. But there is this limiting wall. What has happened in baseball is that all aspects of play have improved enormously. Back in 1911, average play was so far inferior to where Ty Cobb was that his batting average could be measured as .420. Today, Tony Gwynn is just as good, maybe even closer to the wall than Cobb was. But the average player has improved so much that Gwynn's performance -- equal to or better than Cobb's -- is not measured as high.

In sum: It should be obvious that player performance has improved over time in all aspects -- hitting, pitching, and fielding. The reason we don't see .400 hitters anymore is not that players have gotten worse, or even that pitchers have gotten better faster than hitters. It's that as players collectively approach the right limit of human achievement in all aspects of the game, variation shrinks. Cobb and Hornsby were able to put up such gaudy numbers because they were simply closer to this point before everyone else caught up.

###Evaluating Gould's Hypothesis

First, I would like to test Gould's central argument:

> "The overall batting average has been about .260 throughout the history of baseball. But the variation around that average has shrunk.""

To test the first claim, I simply look at raw league-level batting averages by season.


```{r echo = FALSE}
#Simple average: Total Hits / Total At Bats
league_level_bstats <- bstats %>% group_by(yearID,lgID) %>% 
                                  summarize(AB = sum(AB,na.rm=TRUE),
                                            H = sum(H,na.rm=TRUE),
                                            ba = H/AB)



ggplot(league_level_bstats,aes(x = yearID,y = ba,col = lgID)) + geom_line() + 
       theme_fivethirtyeight() + 
       scale_y_continuous(breaks = c(.240,.250,.260,.270,.280,.290,.300,.310),
                          labels = c('.240','.250','.260','.270',
                                     '.280','.290','.300','.310')) +
      ggtitle("MLB Batting Average Over Time") 




```

At first glance, Gould's claim appears to be a bit of an overstatement, in particular brushing aside the sustained level of .280+ averages in the 1920's which saw seven of our twelve post-1903 .400 seasons. Throw in Ty Cobb and Shoeless Joe Jackson's .400 seasons during the 1911 and 1912 average spike, and Bill Terry's .401 when the NL average peaked at .303 in 1930, and at least some of their greatness does appear attributable to an increase in averages. 

At the same time, Ted Williams hit his .406 when the major league average sat at .262, a fairly pedestrian level historically. Clearly the decrease in average is not enough to explain the complete absence of .400 hitters over the following 75 years.

To test Gould's second claim, that variation has been decreasing, I've charted the yearly standard deviation of batting averages for players who had at least 400 plate appearances in a season.


```{r echo = FALSE}
yearly_bstats <- bstats %>% filter(PA > 400)
yearly_bstats <- yearly_bstats %>% group_by(yearID) %>% summarise(
                                              num_qualified = n(),
                                              ba_std = sd(BA),
                                              ba_avg = mean(BA))


ggplot(yearly_bstats,aes(x=yearID,y=ba_std)) + geom_point() + geom_smooth() +
      theme_fivethirtyeight() + ggtitle("MLB-wide BA Standard Deviation") +
      xlab("Year") + ylab("Batting Average Standard Deviation")


```

Gould appears unambiguously correct here, though the trend seems nearly complete by about 1950. Another way to visualize this is that, if true variance is decreasing, we should see a decrease in the difference between the league batting average leader and the league average over time as well.



```{r echo = FALSE}
ba_league_leaders <- bstats %>% filter(PA > 400) %>% 
                                group_by(yearID) %>% 
                                slice(which.max(BA)) %>% 
                                ungroup() %>% 
                                select(ba_leader = playerID,
                                       ba_leader_last = nameLast,
                                       ba_leader_first = nameFirst,
                                       yearID,
                                       ba_leader_ba = BA)


yearly_bstats <- left_join(yearly_bstats,ba_league_leaders, by = "yearID")

```


```{r echo = FALSE}


ggplot(yearly_bstats,aes(x = yearID,y = ba_avg)) + geom_line() +stat_smooth() + theme_wsj() +ylim(.200,.450) +geom_point(data = yearly_bstats,aes(x=yearID,y=ba_leader_ba)) + stat_smooth() + theme_fivethirtyeight()+ ggtitle("Batting Average Leader vs. MLB Average") + ylab("Batting Average") + xlab("Year")



```

Hard to say initially, but it does appear that league fall faster between 1900 and 1950 than do  league wide averages. It's easier to see this trendwhen we plot the spread (BA leader - league average) over time.

```{r echo = FALSE}
yearly_bstats$ba_max_spread <- yearly_bstats$ba_leader_ba - yearly_bstats$ba_avg

ggplot(yearly_bstats,aes(x = yearID,y = ba_max_spread, label = ba_leader)) + geom_point() +stat_smooth() + theme_fivethirtyeight() +ylim(0,.200) +ggtitle("Spread between League Leader and Average") + ylab("League Leader BA - Average BA")
```

Between 1900 and 1950, the difference between the typical league leader and the league average fell by about 50 points.

###Redefining Greatness

This brings me to my central: If we measure performance ***relative*** to one's time not only in terms of average but ***also in terms of variance***, how does the table of greatest seasons change? Who stood the farthest away from their peers, in terms of standard deviation, in a given season? Below, I create a new table of the top 12 seasons of all time by the number of standard deviations their batting average was away from the league.


```{r echo = FALSE}

yearly_bstats <- yearly_bstats %>% mutate(std_away_ba = (ba_leader_ba - ba_avg)/ba_std)

greatest_seasons_redux <- yearly_bstats %>% 
                          arrange(desc(std_away_ba)) %>% 
                          mutate(name = paste0(ba_leader_first," ",ba_leader_last)) %>%
                          select(name,std_away_ba,yearID, ba_leader_ba)




kable(greatest_seasons_redux[1:12,],caption = "Twelve Greatest Batting Seasons since 1903",
      col.names = c("Name", "Standard Deviations Away", "Season", "Raw Average"))

```

The .400 season is out! The 3.5 standard deviation season is in!

Overall, it does appear that our early twentieth century greats (Cobb, Hornsby, Sisler) benefited from both higher average environments of their time as well as a high level of variance in play.

####Biggest Winners:

**Rod Carew and George Brett:** The only players to break the 4.0 standard deviation barrier in baseball history. Well-known to baseball fans, but certainly not household names on the level of Ted Williams or Ty Cobb.
    
**Ted Williams:** Not only is Ted Williams right behind Carew and Brett, with his greatest season moving from 7th place to third, his less-known .388 season in 1957 (16 years later!!!) winds up placing 5th on our new list as well.

**Wade Boggs:** Wade Boggs ends up cracking the list not once but twice, with relatively pedestrian .366 and .368 averages. 
    
####Greatest Losers 

**George Sisler, Ty Cobb, Rogers Hornsby:**

While Cobb and Hornsby still crack the list one time each, they composed 8 of the 12 seasons in our original list. Clearly players who were way ahead of their time, but also in an era in which it was **easier to be way ahead of your time**. 

As a final sanity check,  and one reason I like this method, compare the time distribution of the top 12 raw batting average seasons vs. the top 12 by standard deviation.


```{r echo = FALSE}

std_best <- ggplot(greatest_seasons_redux[1:12,],aes(x = yearID, y = std_away_ba, label = name)) + geom_text() + ggtitle("Top Twelve Batting Seasons by Standard Deviation") + xlim(1900,2015)

raw_best <- ggplot(greatest_seasons[1:12,],aes(x = yearID, y = BA, label = name)) + geom_text() + ggtitle("Top Twelve Batting Seasons by Raw Average") + xlim(1900,2015)

grid.arrange(std_best,raw_best)

```

By looking at standard deviations, we get a much more even spread of greatness over time. This seems more fair, and gives more justice to the fantastic seasons of Brett, Carew, Boggs, Bonds, and Ichiro. 

[^1]: 1903 is the first year that both the NL and AL adopted the fouls-as strikes rule. Nap Lajoie was able to take advantage of unlimited foul balls in 1901 to hit .426
